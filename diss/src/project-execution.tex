\chapter{Project Execution}
\vspace{1cm}

\todo{Say what my project actually is.}
My project is a tag-based FUSE file system (from now on creatively referred to
as ``\texttt{tagfs}''). Tagfs is written in the Rust programming language and
relies on the ``fuser'' library to communicate with and decode messages from
the FUSE kernel module. Tagfs also makes use of the SQLite database library
through the Rust wrapper interface provided by ``rusqlite''. Specific features
of tagfs are discussed in more detail in Section \ref{section:features}.
\todo{This feels like maybe it should be part of the introduction?}
\todo{Talk about what exactly a tag is?}

\section{Design}
\label{sec:design}

\subsection{Intended Users}

\todo{Talk about who would use the software i.e. power users etc.}

When developing a software project it is important to keep the intended users
of the project in mind as much as possible throughout the development of the
project. This is to ensure that the project does not lose focus and make poor
design decisions that will negatively impact the intended user base. For
\texttt{tagfs} I decided that my intended users would be technically
minded and capable users that are comfortable and feel at home on the command
line. By aiming for technical users, I was able to focus on developing the
main functionality of the software rather than spending time developing a user
interface that is more suitable for less technical users. However, this did not
mean that I did not want to provide a good user experience for my user base. To
provide a good user experience for technical users, I decided to create a
command line interface to interact with \texttt{tagfs}. Additionally, I wanted
to ensure that the CLI followed common interface designs and patterns seen in
other CLI programs. This is to help the user feel at home quickly if they have
experience with other popular CLI programs. Therefore, I decided to create a
subcommand style CLI similar to \texttt{git} and \texttt{cargo} (the Rust build
tool).

Additionally, I decided to restrict my user base to Linux users. This meant
that the development process would be much easier due to only having to develop
for and test on one platform. This way also meant that I could focus on the
FUSE technology rather than having to create a wrapper between FUSE and the
Windows equivalent. I felt that restricting the user base in this way would not
be too confining because I thought that many of the technical users that I was
already targeting would be using a Linux platform anyway.

\subsection{Technology Choices}

Choosing suitable technology is a vital component of a successful software
project. Unsuitable technology can result in a project being much more complex,
brittle, and unmaintainable than it otherwise could have been. \todo{source?
(dude just trust me)}

I chose Rust as the implementation language for my project. One of the factors
that pushed me towards Rust was my prior experience creating small hobby
projects using the language. In these smaller past projects I had found the
features of Rust, such as its strong static type system and focus on
correctness, to lend themselves to producing robust and maintainable systems.
Additionally, the potential performance of a Rust solution (due to Rust's lack
of a runtime garbage collector) attracted me, because file system code requires
good performance to avoid frustrating the user or slowing down other programs
that depend on it. However, I did have some concerns about the choice of Rust.
My main concern was, due to the fact that Rust is a relatively young
programming language\footnote{Rust 1.0 was released in 2015
(\url{https://blog.rust-lang.org/2015/05/15/Rust-1.0.html}) most other widely
used programming languages were first released at least 20 years earlier.},
there would not be many high quality libraries available. It was a pleasant
surprise then that when browsing the Rust community repository
crates.io\footnote{\url{https://crates.io/}}, I found many projects with active
communities that appeared to be reasonably mature and usable. My heuristic for
identifying these libraries is based on the number of downloads on crates.io
and the last commit date on GitHub. I found that libraries with a high number
of downloads tended to represent the best the community had to offer in that
particular category. Libraries with lots of recent commits tend to have more
active communities and that often means that there are people asking questions
and receiving answers which is useful to fill in any gaps in the documentation.

The other programming language that I considered for my project was C. The main
reason that I considered C was the fact that the reference implementation of a
FUSE library (libfuse\footnote{\url{https://github.com/libfuse/libfuse}}) is
written in C and most tutorials and documentation assume the use of C. Using C
would have meant that I would have a much wider array of examples to help me
implement \texttt{tagfs}. However, I decided against the use of C because I
felt that I would miss the higher level of abstraction provided by other
languages with features such as generics and tagged unions built into the
language. Additionally, I did not want to deal with painfully manual memory
management and I did not feel confident in producing code without memory
safety bugs such as use after free and segmentation faults. These memory safety
bugs can be somewhat prevented through the use of modern sanitisers such as
ASAN and UBSAN, but these sanitisers are only effective at runtime and only if
the pathological case that causes the bug appear. Rust on the other hand can
catch these bugs at compile time and prevent them from ever occurring.

\todo{Why is Rust a suitable programming language for my project?
Alternatives? Why Rust not C?}

\subsubsection{Data Storage Back-end}

Early on in the planning phase of my project I knew I was going to need some
mechanism to persist data to the file system between mounts of the file system.
I knew my data would be at least vaguely relational, because I knew that I
needed to model the relationship between a file system path and the tags
associated with it. This is what initially drew me to a database solution
rather than a simpler flat text file. However, the main reason I chose to use
an SQL database was the ability to constrain my data and enforce invariants at
the database level. While it is possible to do this with a flat file approach,
I believed that this would be difficult to implement and less performant than
using a database.
Additionally, it made sense to use an off-the-shelf solution because the
particular underlying data store is not directly relevant to the goals of the
project and therefore I could have spent a lot of time reinventing the wheel
rather than creating \texttt{tagfs}.

Once I had decided that I was going to use a database, my choice of which
database implementation to use was very easy. SQLite is the most deployed
database software on the planet \cite{sqlite-most-used}, and it is perfect for
my use case of a local data store. It is fast, battle-tested, and well
documented. Unsurprisingly, there is a Rust wrapper library for
SQLite\footnote{This highlights another important Rust feature: its foreign
function interface (FFI)}. Rusqlite packages up SQLite into a more Rust-like
interface suitable for use from entirely safe code.

\todo{Why did I choose SQLite? Alternatives? Flat File? Own Format? NoSQL? ORM?}

\subsection{Database Schema Design}

After having decided to use an SQL database, I needed to design a schema for
each table in the database. I settled on two tables initially: one to store the
tags and one to store the mapping of tags to paths. The second table contains a
foreign key field that references a tag in the first table.
The reason I decided to have a table entirely dedicated to storing tags was to
enforce a consistent usage for each tag. This means that for every usage of a
tag, the tag must always be given with or without a value, but it must be
consistent across all uses of a tag. If I did not have a dedicated table to
restrict each tag to one usage style, then each usage of a tag could have
potentially different behaviour which would be confusing.

I also use the ``trigger'' feature of SQL to ensure that tags that are no
longer used by any path are removed from the database (the trigger code is
shown in Figure~\ref{fig:sql-trigger}). This helps to avoid the problem of
having unused tags in the output of the CLI and file system. The trigger is run
whenever a row is deleted in the ``TagMapping'' table. When the trigger is run
a ``DELETE'' SQL command is issued that searches for any usage of the deleted
tag in the ``TagMapping'' table, if there is no usage found then the tag is
removed from the ``Tag'' table.

% trigger code {{{
\begin{figure}[h]
    \centering
    \begin{boxedverbatim}

CREATE TRIGGER IF NOT EXISTS
RemoveUnusedTags AFTER DELETE ON TagMapping
BEGIN
  DELETE FROM Tag
  WHERE
    Tag.TagID = OLD.TagID
    AND NOT EXISTS(
      SELECT TRUE
      FROM TagMapping
      WHERE TagMapping.TagID = OLD.TagID);
END
    \end{boxedverbatim}
    \caption[SQL Trigger Code]{SQL trigger code to remove tags that are no
    longer referenced from the database.}
    \label{fig:sql-trigger}
\end{figure}
% }}}

\todo{Maybe include an Entity Relationship Diagram?}
\todo{Talk about not over-normalising data?}

\section{Features}
\label{section:features}

Tagfs exposes two different interfaces for the user to interact with: a CLI and
a file system interface. The CLI uses both the classic UNIX style options
(\texttt{--help} or \texttt{--database}) and a more modern (at least as far as
CLIs go) subcommand style interface similar to the \texttt{git} CLI.

% tagfs CLI {{{
\begin{figure}[h]
    \centering
    \begin{boxedverbatim}
$ tagfs help
Tag-based virtual file system backed by SQLite.

Usage: tagfs [OPTIONS] <COMMAND>

Commands:
tag             Apply a tag to a path
untag           Remove a tag from a path
mount           Mount the filesystem [aliases: mnt, m]
tags            Display tags associated with a path
query           Query the database [aliases: q, search]
autotag         Autotag a directory tree or file
prefix          Modify the prefix of paths in the database
edit            Edit the tags database using a text editor
stored-queries  List, create and delete stored queries in the database
help            Print this message or the help of the given subcommand(s)

Options:
    --database <database>  Path to database to use or create
-h, --help                 Print help
-V, --version              Print version
    \end{boxedverbatim}
    \caption[Help for \texttt{tagfs} CLI]{Help for tagfs. Shown when the
        \texttt{tagfs} or \texttt{tagfs help} command is run.}
    \label{fig:root-cli}
\end{figure}
%stopzone stops vim syntax highlighting running over.
% }}}

Each subcommand acts as if it is its own self contained program with its own
options and arguments. However, there are some options that all of the
subcommands share, these can been seen at the bottom of
Figure~\ref{fig:root-cli}. The \texttt{--database} option specifies the SQLite
database to use for a specific subcommand. If this option is given, but the
database does not exist at the given location then a new database is created at
the specified location. If this option is not given at all then, following the
XDG base directory specification \cite{xdg-base-dir-spec}, a sensible default
location is found. If a database is found at this location then it is used, but
if there is no database then an empty one is created.

The \texttt{tag} and \texttt{untag} subcommands are both straightforward CRUD
actions in the database, create and delete respectively. Both subcommands take
a path and a list of tags. The \texttt{tag} subcommand tags the given path with
each tag in turn. The \texttt{untag} subcommand removes each given tag from the
given path. If the list of tags given is empty then \texttt{untag} removes all
tags from the given path.

The \texttt{mount} subcommand mounts the FUSE file system at the given mount
point. The FUSE file system is discussed in more detail in
Section~\ref{sec:fs-features}.

The \texttt{tags} subcommand lists all of the tags stored on a given path. The
information provided by this command is also exposed through the FUSE file
system. The situation is similar for the \texttt{query} subcommand. The
subcommand takes a query and lists all of the paths that match. Queries are
discussed in more detail in Section~\ref{sec:queries}.

The \texttt{autotag} subcommand takes a path. If the path is a directory then
the subcommand will recursively search the directory for all its descendants
that are files. Each of these files is checked to see if they are of a type
that the \texttt{autotag} subcommand recognises. If \texttt{autotag} recognises
a file's type then it will attempt to extract or generate tags based on the
file's content. For example, if \texttt{autotag} comes across an MP3 file it
will try and extract the file's metadata tags and build tags based on them. The
tool is able to extract album, album-artist, title, and date information from
music files. The software also recognises image files and can extract the time
the image was taken from the file's metadata. Additionally, the tool can also
handle video files. If the tool detects a video file (matroska or MP4) named
with the common convention of ``\texttt{Title (Year)}'' then it will assume the
file is a film and will search TMDB\footnote{\url{https://themoviedb.org}} for
related metadata. The tool uses the TMDB API to access information such as the
film's genres and the top billed actors and will then store this information as
tags in the database.

The \texttt{prefix} subcommand is used for searching and replacing components
of the paths stored in the database. It takes two strings as arguments: the
term to search for and the term to replace it with. It then scans the database
for any paths matching the search term and replaces the match with the given
replacement text. This subcommand is designed to reduce the burden of updating
the database after you have moved some files. For example, you might have moved
your music directory to another location on your system, and using the prefix
command you can update all of the paths for your music files to use the new
location in one go.

The \texttt{edit} subcommand is used to make fine-grained edits to the database
with the comfort of a familiar interface. This subcommand dumps the contents of
the database to a temporary file in a human-readable format. Then the
subcommand invokes the user's preferred editor\footnote{It finds the user's
preferred editor by first reading the \texttt{\$VISUAL} environment variable and
failing that it tries the \texttt{\$EDITOR} environment variable before falling
back to a default value.} on the temporary file and waits for the editor
process to terminate. Once the editing session is complete, the \texttt{edit}
subcommand checks to see if the user has made any changes to the temporary
file and, if they have, commits them to the database.

% edit subcommand format figure {{{
\begin{figure}[h]
    \centering
    \begin{boxedverbatim}


--------
/path/to/tagged/file
tag1=value
tag2
tag3=long\ value
--AUTO--
autotag=autovalue
--------

--------
/path/to/another/tagged/file
tag1=other-value
tag2
--AUTO--
another-autotag=autovalue
--------
    \end{boxedverbatim}
    \label{fig:edit-subcommand-format}
    \caption{Example of the \texttt{edit} subcommand editing session.}
\end{figure}
% }}}

Figure~\ref{fig:edit-subcommand-format} shows a potential editing session. The
file is split into blocks delimited by eight dashes. Each block represents one
path in the database. The path is specified as the first non-empty line inside
the block. Each non-empty line after that specifies a single tag to be applied
to the path given at the start of the block. The tags are read in the exact
same way as a query (more detail about query syntax in
Section~\ref{sec:queries}) with the additional constraint of only being
allowed to contain one tag and an optional value. There is also the possibility
of an `\texttt{--AUTO--}' heading somewhere in the block. Any tags read after
this special heading, are marked as being generated by the \texttt{autotag}
subcommand. This is useful to separate user defined tags from automatically
generated ones. This separation is important so that the \texttt{autotag}
subcommand knows which tags are safe to overwrite. The user may make changes to
any part of the temporary file, including removing entire blocks, removing the
`\texttt{--AUTO--}' special heading, and changing the path entirely.

The \verb@stored-queries@ subcommand is a simple interface to manage the
queries that are visible by default in the file system. It allows the creation
of new stored queries, the deletion of existing stored queries and the listing
of all queries stored in the database. How the stored queries are used and
displayed is explained in more detail in Section~\ref{sec:fs-features}.

The final subcommand is the \verb@help@ subcommand. As seen in
Figure~\ref{fig:root-cli}, the \verb@help@ subcommand prints out a static
message informing the user how to use the software and what options the
software has available. The \verb@help@ subcommand is also able to take another
subcommand as an option and then it will display the specific help and options
for that particular subcommand.

\subsection{Queries}
\label{sec:queries}

% query grammar {{{
\begin{figure}[t]
\begin{grammar}
<query> ::= <term> <whitespace> <binary-bool-op> <whitespace> <query>
    \alt <unary-bool-op> <whitespace> <query>
    \alt `(' <maybe-whitespace> <query> <maybe-whitespace> `)'
    \alt <term>

<term> ::= <tag> <maybe-whitespace> <comp-op> <maybe-whitespace> <value>
    \alt <tag>

<tag> ::= <tag-char> <tag> | <tag-char>

<tag-char> ::= \{Set of non-whitespace, non-control Unicode characters\}
    $\setminus$ \{`=', `<', `>', `(', `)'\}

<value> ::= `"' <value-quoted> `"' | <value-unquoted>

<value-quoted> ::= <value-char-quoted> <value-quoted>
    | <value-char-quoted>

    <value-char-quoted> ::= \{`\\"'\} $\cup$ \{Set of non-control Unicode
        characters\} $\setminus$ \{`"'\}

<value-unquoted> ::= <value-char-unquoted> <value-unquoted>
    | <value-char-unquoted>

<value-char-unquoted> ::= \{ `\\ ', `\\"', `\\)' \} $\cup$ \{Set of
    non-whitespace, non-control Unicode characters\} $\setminus$ \{`)', `"'\}

<comp-op> ::= `>' | `<' | `=' | `=='

<binary-bool-op> ::= `and' | `or'

<unary-bool-op> ::= `not'

<whitespace> ::= ` ' <whitespace> | ` '

<maybe-whitespace> ::= <whitespace> | $\epsilon$

\end{grammar}
\label{fig:query-bnf}
\caption[BNF description of the query language of \texttt{tagfs}]
    {BNF description of the query language of \texttt{tagfs}. Note that set
    notation is used as a shorthand to describe production rules with many
    terminals. For example the set $\{a, b, c\}$ produces the rule $a \vert b
    \vert c$. \todo{note that tags cannot have the same name as boolean
    operators}}
\end{figure}
% }}}

Whilst using tagfs there are multiple contexts in which the user may be asked
for a query. These are when using the FUSE file system and when using the
\texttt{query} subcommand. A query is an expression which any particular path
in the database either satisfies or does not satisfy.

At the most basic level, a query can contain a single tag, for example
\texttt{favourite}. This query matches all the paths in the database that have
been tagged with the tag ``favourite''. This is useful on its own, but not very
powerful. However, it is possible to combine multiple tags into a single query
with boolean operators. One such operator is the \texttt{and} operator. It
matches a path if both its operands match. For example, the query
\texttt{favourite and watched} matches all the paths tagged with both
``favourite'' and ``watched''. Additionally, there is a union operator
\texttt{or} that matches a path if either or both of its operands match. A
query can also be inverted by using the \texttt{not} operator. For example, the
query \texttt{not favourite} matches all paths in the database that are not
tagged with ``favourite''. These operators can be combined in many ways to
produce arbitrary expressions. For example, the query \texttt{not favourite and
(not watched or recent)} matches any paths that do not have the ``favourite''
tag and have the ``recent'' tag or don't have the ``watched'' tag. In the
previous query parentheses are used to group parts of the expression
differently than without. Query expressions follow the conventional precedence
rules that specify that parentheses have the highest precedence, followed by
\texttt{not}, followed by \texttt{and}, followed by \texttt{or}.

In the database it is possible for tags to be associated with values, so the
query language supports matching on a tag's value as well as just the tag's
presence. Query expressions can be formed by using a comparison operator along
with a tag and a value. The most common comparison operator is \texttt{=} it is
used to test whether a tag's value contains a particular substring\footnote{By
default the \texttt{=} operator is not case sensitive. This can be adjusted via
a flag on the \texttt{query} subcommand.}. For example, the query
\texttt{title=before} matches any paths that have a tag called ``title'' with a
value containing the string ``before''. Another comparison operator is
\texttt{==} (strict or exact equals) this is similar to the standard \texttt{=}
operator except the given value must be exactly the same as the value attached
to the tag in the database for it to match. The final two comparison operators
are the less than \texttt{<} and greater than \texttt{>} operators. These match
a path if the path has the given tag and the tag's value is less than or
greater than (depending on the operator used) the value given in the query. The
ordering used to determine whether a value is less than or greater than another
is lexicographical. This can produce behaviour that may be surprising such as
the fact that `$2$'~$>$~`$10$'. These comparison operators can of course be
combined with the earlier described boolean operators to create more complex
query expressions. The values used with the comparison operators must be
escaped properly to ensure the query's semantics. For example, the query
\texttt{genre=science fiction} is invalid because values are terminated by a
space character, so this is parsed as two separate tags ``genre'' and
``fiction''. Since there is no comparison operator between them, the query is
invalid and is therefore rejected. To achieve the desired meaning the value
associated with a tag can be escaped or quoted. For example, to query for a
path with the tag ``genre'' and value ``science fiction'' the two forms
\verb_genre=science\ fiction_ and \texttt{genre="science fiction"} can be used
and both have the same meaning. The query syntax is specified more precisely in
Figure~\ref{fig:query-bnf}
using BNF notation.

\subsection{File System Features}
\label{sec:fs-features}

% Sample tagfs FUSE file hierarchy figure {{{
\begin{figure}[p]
\centering
\begin{forest}
    for tree = {%
        folder,
        grow'=0,
        fit=band,
        font=\ttfamily,
        s sep=0.0mm,
    }
    [fuse-mount-root/,color=teal
        [?,color=teal
            [{favourite-crime @ [type=film and genre=crime and
            favourite]},color=teal
                [\textcolor{purple}{Heat (1995)} -> \textcolor{teal}{/film/Heat
                (1995)/}]
            ]
        ]
        [tags,color=teal
            [film,color=teal
                [Heat (1995).tags,color=olive]
            ]
            [music,color=teal
                [The Smashing Pumpkins,color=teal
                    [Siamese Dream,color=teal
                        [01 Cherub Rock.flac.tags,color=olive]
                        [...,color=black]
                    ]
                ]
            ]
        ]
        [favourite,color=teal
            [\textcolor{purple}{Heat (1995)} -> \textcolor{teal}{/film/Heat
            (1995)/}]
        ]
        [genre,color=teal
            [crime,color=teal
                [\textcolor{purple}{Heat (1995)} -> \textcolor{teal}{/film/Heat
                (1995)/}]
            ]
            [rock,color=teal
                [\textcolor{purple}{01 Cherub Rock.flac} ->
                    \textcolor{olive}{/music/The Smashing Pumpkins/Siamese
                    Dream/01 Cherub Rock.flac}]
                [...,color=black]
            ]
        ]
        [type,color=teal
            [film,color=teal
                [\textcolor{purple}{Heat (1995)} -> \textcolor{teal}{/film/Heat
                (1995)/}]
            ]
            [music,color=teal
                [\textcolor{purple}{01 Cherub Rock.flac} ->
                    \textcolor{olive}{/music/The Smashing Pumpkins/Siamese
                    Dream/01 Cherub
                    Rock.flac}]
                [...]
            ]
        ]
        [year,color=teal
            [1995,color=teal
                [\textcolor{purple}{Heat (1995)} -> \textcolor{teal}{/film/Heat
                (1995)/}]
            ]
            [1993,color=teal
                [\textcolor{purple}{01 Cherub Rock.flac} ->
                    \textcolor{olive}{/music/The Smashing Pumpkins/Siamese
                    Dream/01 Cherub Rock.flac}]
                [...]
            ]
        ]
    ]
\end{forest}
\caption[Sample \texttt{tagfs} FUSE file system hierarchy]
{Sample \texttt{tagfs} FUSE file system hierarchy with directories in
\textcolor{teal}{blue}, files in \textcolor{olive}{green}, and symbolic links
in \textcolor{purple}{pink}. An arrow (`\texttt{->}') following a symbolic link
shows the target that the link is referring to.}
\label{fig:sample-tagfs-tree}
\end{figure}
% }}}

% Arbitrary query tagfs FUSE file hierarchy figure {{{
\begin{figure}[p]
\centering
\begin{forest}
    for tree = {%
        folder,
        grow'=0,
        fit=band,
        font=\ttfamily,
        s sep=0.0mm,
    }
    [fuse-mount-root/?/{genre=rock},color=teal
        [\textcolor{purple}{01 Cherub Rock.flac} ->
            \textcolor{olive}{/music/The Smashing Pumpkins/Siamese
            Dream/01 Cherub
            Rock.flac}]
        [...]
    ]
\end{forest}
\caption[Sample result of looking up an arbitrary query directory.]
{Result of looking up an arbitrary query directory with directories in
\textcolor{teal}{blue}, files in \textcolor{olive}{green}, and symbolic links
in \textcolor{purple}{pink}. An arrow (`\texttt{->}') following a symbolic link
shows the target that the link is referring to.}
\label{fig:sample-tagfs-query-dir}
\end{figure}
% }}}

% sample contents of a .tags file {{{
\begin{figure}[p]
    \centering
    \begin{boxedverbatim}


favourite
genre=crime
type=film
year=1995
    \end{boxedverbatim}
    \caption[Example contents of a \texttt{.tags} file]{Contents of the file
        \file{fuse-mount-root/tags/film/Heat (1995).tags} from
        Figure~\ref{fig:sample-tagfs-tree}}
    \label{fig:tagfs-all-tags-example}
\end{figure}
% }}}

Once the file system has been mounted using the \texttt{tagfs mount} subcommand
it can be interacted with as if it were any other file system on the system.
Figure~\ref{fig:sample-tagfs-tree} shows a possible hierarchy for the FUSE file
system. Within the root directory (in Figure~\ref{fig:sample-tagfs-tree} this
is \texttt{\textcolor{teal}{fuse-mount-root/}}) a directory is created for each
tag in the database. In Figure~\ref{fig:sample-tagfs-tree} these tag
directories are \dir{favourite}, \dir{genre}, \dir{type} and \dir{year}. Each
tag directory has different children based on whether the tag is associated
with a value or not. If a tag does not have an associated value (this is the
case for the \dir{favourite} tag directory in
Figure~\ref{fig:sample-tagfs-tree}) then the corresponding tag directory will
contain a symbolic link for each path that shares the tag. Each symbolic link
points to the tagged path and shares the same name as the last component of the
tagged path. An example of one of these symbolic links is \symlink{Heat (1995)}
in the \dir{favourite} directory. Alternatively, if a tag is associated with a
value then the children of the tag directory are the enumerated values
associated with that tag. An example of this type of tag directory is
\dir{genre} with two associated values represented by the value directories
\dir{crime} and \dir{rock}. Each of these value directories contains symbolic
links to the paths that are tagged with the tag of the parent tag directory and
the value of the value directory. These two types of directories provide an
easy to access method of performing simple queries of the database for a single
tag. However, it is desirable to be able to perform more advanced queries from
the file system.

This is where the query directory (\dir{?/}) is useful. The query directory
contains sub-directories representing each of the stored queries saved by the
\texttt{stored-queries} subcommand. The stored query directories are named with
the format \texttt{<query-name> @ [<query>]}. Each stored query directory
contains the results of executing the stored query the directory represents. In
Figure~\ref{fig:sample-tagfs-tree} the stored query directory is named
\texttt{favourite-crime} and contains the results of executing the
\texttt{type=film and genre=crime and favourite} query, which in this example
is one path represented by the symbolic link \symlink{Heat (1995)}. Stored
queries are useful for saving frequently used queries, but it would be very
restricting to only be able to access the results of queries previously stored
in the system. To make running one-off queries less tedious, the query
directory also supports running arbitrary queries by looking up
`hidden' directories\footnote{These directories are not actually hidden, rather
they do not actually exist until they are looked up.}. These directories are
children of the query directory (\dir{?/}) named after the query they
represent. Using the tags established in Figure~\ref{fig:sample-tagfs-tree} as
an example, looking up the directory \dir{?/genre=rock/} would result in a
directory containing the results of the \texttt{genre=rock} query, which in
this case would be a symbolic link named \symlink{01 Cherub Rock.flac} pointing
to the target \file{/music/The Smashing Pumpkins/Siamese Dream/01 Cherub
Rock.flac}. This is illustrated in Figure~\ref{fig:sample-tagfs-query-dir}.

The file system also exposes another special directory: the \dir{tags}
directory. The \dir{tags} directory provides a method of viewing all of the
tags associated with a particular path. Under the \dir{tags} directory a
structure mirroring the hierarchy of all the paths in the system is created.
Browsing this structure is just like browsing the normal file system hierarchy
outside of \texttt{tagfs}. However, the last component of the path is replaced
by a file that has the same name as the last component of the path with an
extra \texttt{.tags} extension. An example of this hierarchy can be seen in
Figure~\ref{fig:sample-tagfs-tree}. Inside each of these \texttt{.tags} files
is a list of all the tags on the path represented by the file. For example,
the file \file{fuse-mount-root/tags/film/Heat (1995).tags} contains a list of
all the tags on the path \texttt{/film/Heat (1995)}. The particular tags for
this example are shown in Figure~\ref{fig:tagfs-all-tags-example}.

The file system is also able to handle modifications to the database whilst it
is mounted. This means that whenever the database is modified, the file system
will adjust to the new database state seamlessly. As such, mounting and
remounting the file system to handle changes is completely unnecessary.

\section{Implementation}
\label{sec:implementation}
\todo{Talk about order of development, e.g. making empty fs first etc...}
\todo{Talk about how core functionality is implemented as a library separate to
    the cli.}

The \texttt{tagfs} project has two main components: the FUSE file system and
the CLI tool. There is a significant overlap in functionality between the two
components, therefore it made sense to only implement functionality once and
share the code between the two components. To do this I developed the shared
functionality as a separate library.
The CLI program is then a separate binary that is statically linked to this
library and the file system is a sub-module within the library. This separation
ensures that the CLI can operate on the database and mount the file system, but
only through the API exposed by the shared library. Separating the concerns of
the software (separating the user interface from the core functionality) also
makes the software more easily testable. This is discussed in more detail in
Section~\ref{sec:testing}.

When I began to start implementing \texttt{tagfs} (after having done some broad
design work) I had to decide which part of the project to work on first. I
decided to first work on the part of the project which I anticipated would be
the most the difficult. This was the file system portion of the project. I
decided to work on, what I thought would be, the most difficult part of the
project first to ensure that it was achievable. I did not want to leave a
critical, but difficult, part of the project until much later on and find out
that it was not possible or that I could not implement it satisfactorily.
Following this strategy, I started by implementing a file system that could
simply be mounted and contained no files or directories. Then building on this
prototype, I created a file system that contained a static list of empty
directories. Next I used a hash map structure to represent a file system
containing a series of directories that each have their own children (which
were limited to symbolic links for simplicity). This iterative development
process formed the basis for all progress in the project and allowed me to
increase the complexity of the project gradually rather than trying to build
the entire thing at once which would have been overwhelming.

After creating a simple file system based on hard coded values, I decided to
try and connect the file system to a database to allow for non-hard-coded paths
in the file system. I wanted to ensure that any direct communication with the
database via SQL was abstracted away into a separate module to avoid
``polluting'' other parts of the code with implementation details specific to
the database. This made the file system code easier to read due to the
abstraction of the database and has the additional benefit of making the
database code easier to reuse. After connecting the database to the file
system, I needed a way to modify the database so I started to work on the CLI.
I added a simple interface to add and remove tags from the database by using
and extending the database module I previously developed for the file system. I
decided not to handle command line argument parsing myself and instead used the
popular ``clap\footnote{Clap is an acronym for ``command line argument parser''
and can be found at the following url \url{https://crates.io/crates/clap}}''
library to build a command line interface in a declarative fashion. Clap
integrates with the Rust documentation system to generate command line help
messages from a single source of truth. This source of truth is the special
documentation comments within the structures and enums used by clap to build
the CLI. This makes it extremely easy to keep both the documentation and
command line help in sync, and makes it easy to remember to update the
documentation as it is right next to the corresponding code. \todo{Defo Maybe
example of declarative nature of clap arg parsing?}

Next I created the concept of queries within \texttt{tagfs} and decided to make
a mini DSL to specify them. This required me to create a tokeniser that splits
a given query into a stream of tokens. This stream of tokens can then be
converted to SQL via another function. I then packaged these functions into a
sub-module of the database module, so that they could be used from the CLI (via
a dedicated subcommand) and from the file system. When developing the query
sub-module, I decided to simplify the implementation by using the principle of
"Rubbish in, rubbish out" such that if the user gave a malformed or nonsensical
query then it would be allowed for the query builder to output nonsensical or
malformed SQL. My theory was that generating invalid SQL was not a problem
because it would be caught and rejected by SQLite. This greatly simplified the
query builder because it meant that I would not have to write a fully fledged
parser and SQL generator for the query language. However, there were downsides
to this approach such as the inability for \texttt{tagfs} to tell the user
where the error in an invalid query is because \texttt{tagfs} does not have
this information. I think this was a fair trade-off, because queries tend to be
on the shorter side and spotting errors is usually fairly simple.

\subsection{Software Development Practices}

\todo{Talk about use of static analysis - clippy.}
\todo{Talk about use of cargo doc.}
\todo{Talk about conditional compilation of the autotag feature to reduce
dependencies.}
\todo{Talk about how static typing helped me.}

During the development of \texttt{tagfs} I tried to follow good software
development practices. The aim of these practices being to improve the
robustness of the code and to ensure that the code is readable and therefore
maintainable. One of these techniques is static analysis. Static analysis is
used to detect potential problems in the code without actually running it
\todo{Do I need a source here?}. Rust has a first party static analysis tool
called clippy\footnote{Named after everyone's favourite paper-clip. Can be
found at \url{https://github.com/rust-lang/rust-clippy}}. Clippy can detect
style problems and suggest alternatives, such as transforming iterator chains
that contain a \verb@filter@ followed by a \verb@map@ into a single
\verb@filter_map@ operation. Lots of the clippy style detections are for
seemingly trivial problems (for example preferring the use of \verb@xs.first()@
over \verb@xs.get(0)@) however, in my experience I found that these ``trivial''
problems accumulate over time to make the code more difficult to read. In my
opinion, putting in the effort to correct these small issues is important for
improving the clarity of the code. Additionally clippy can also detect
potential performance issues, for example clippy can detect useless heap
allocations that would be better served by a simple reference to an existing
allocation.

Whilst developing the project, I endeavoured to document the public API
functions of \texttt{tagfs} and some of the internal private functions also.
Rust and the accompanying cargo\footnote{Cargo is the Rust programming
language's build system and package manager, but it also includes an interface
to the rust compiler's built in test runner and documentation builder. The
cargo documentation can be found at
\url{https://doc.rust-lang.org/cargo/index.html}} tooling makes creating and
maintaining documentation simple and hassle-free by keeping the documentation
next to the code that it documents. Code is documented through the use of
special comments, placed directly above the relevant function or module, that
describe what the code should do. When cargo is invoked to build the
documentation using the \verb@cargo doc@ subcommand, all of these documentation
comments are compiled into structured and browsable HTML
documentation\footnote{It should be noted that none of these ideas about
documentation are unique to Rust. The Rust documentation process is heavily
inspired by and very similar to older systems such as javadoc and doxygen.}.
Additionally, cargo will also build the documentation of all the dependencies
of the project to create a cohesive collection of documentation for all code in
the project (this was extremely useful in periods of internet outage).
I found that the act of creating the documentation was more important than the
actual documentation itself. Creating the documentation helped me to think
clearly about what a given module should do and also helped me to discover any
invariants that must be accounted for when calling particular functions. An
example of this is the documentation for the ``fs'' module that helped me to
both document and understand exactly how and in what order the ``fuser''
library will call my code when an ``ls'' is performed in the file system.

Another software development technique I made use of during development was
conditional compilation in the form of cargo
features\footnote{\url{https://doc.rust-lang.org/cargo/reference/features.html}}.
Conditional compilation is a method for compiling the code base in different
ways depending on a set of input parameters. For example, it might be desirable
to compile a particular block of code only on certain platforms - conditional
compilation allows this to be expressed in the source code. \todo{Maybe I need
to properly reference the cargo book?} In \texttt{tagfs} I use conditional
compilation to allow the user to specify, at compile time, whether they would
like to include the autotagging functionality. The way that I achieve this is
by using cargo features to define an ``autotag'' feature and to specify the
additional dependencies that are required to build this feature. Then in the
source code the module that implements the autotagging feature is preceded by
the declaration ``\verb@#[cfg(feature = "autotag")]@'' which tells the Rust
compiler to only compile this next block if the condition is true i.e. the
``autotag'' feature is active. This provides a similar behaviour to
preprocessor macros in C such as \verb@#ifdef@ and \verb@#ifndef@. The main
reasons I chose to increase the complexity of the build process and use
conditional compilation was to provide the user with flexibility regarding the
number of dependencies required to build the code and to allow the user to
disable features that may not be useful for them. I think this is especially
relevant for the autotagging feature, because it is a feature that runs
tangential to the rest of the code and therefore is easy to ``split out''
without causing issues elsewhere in the code base.
This would not be the case for other more fundamental features, such as tags
being able to have an optional value. This feature runs deep throughout the
project and therefore would not be a good target for conditional compilation as
the complexity would be very high.

I also made use of logging to help me develop \texttt{tagfs}. My main usage of
logging was to be able to trace which FUSE functions are called and in what
order when performing a particular file system operation. It is through this
logging that I was able to produce the documentation for how exactly the
``fuser'' library calls my code. When commands such as ``tree'' are run on the
file system, there can be a lot of logging output. To reduce the logging noise,
I used the Rust standard logging package to attach levels to each log
statement, so that, at runtime, the logging level can be specified using an
environment variable. This means that it is easy to control the logging output
without having to tediously guard each logging statement with a conditional.
Additionally, since I used the standard Rust logging package I was able to
integrate my logging with the logs from the libraries used in \texttt{tagfs}.
This meant that I could peer into the logs for code that I depend on and use it
to help me understand what the code was doing, whilst also being able to easily
disable it with one standard mechanism.

\subsection{Testing}
\label{sec:testing}

\todo{Talk about cargo test, and difference between integration and unit
    tests. Talk about TDD for the lexer.}

Whilst implementing \texttt{tagfs} I made sure to test the project's
functionality frequently, both manually and automatically. Manual testing is
initially important to check that a feature seems to work as expected, but
automated testing provides confidence that a feature is working and stays
working throughout changes to the project. To facilitate the creation and
running of tests I followed the standard testing methodology for Rust projects
specified in the Rust book \cite{rust-testing}. This includes the usage of both
unit and integration tests. I used unit tests to isolate and test specific
functions within my project. An example of this is the \verb@sanitise_path@
function that is used to ensure that no two files in the same directory are
given the same file name. Additionally, I found that test driven development
(TDD) was an appropriate and useful technique for developing the lexer
component of the query subsystem. Writing the tests before the implementation
ensured that I had a good grasp of what the lexer needed to achieve and helped
me to foresee tricky cases that the lexer would need to handle.
Figure~\ref{fig:example-unit-test} is a code snippet from one of the tests in
my project. It uses the \verb@assert_eq!@\footnote{In Rust, macros that can be
called like normal functions are terminated with a `\texttt{!}' to help
differentiate them from normal functions.} macro to test equality between its
arguments and to provide a useful error message if the test fails. These tests
were particularly useful when it came to adding the \texttt{==} operator at a
later date to the initial implementation of the query subsystem. It ensured
that the addition of a new operator did not break the existing functionality,
which reduced the fear associated with modifying already working code.

% example lex_query test {{{
\begin{figure}
    \centering
    \begin{boxedverbatim}
assert_eq!(
    super::lex_query("      (     not nothello   = \"(wor\\\"=ld)\""),
    &[
        LeftParen, Not, Tag(String::from("nothello")), Equals,
        Value(String::from("(wor\"=ld)"))
    ]
);
    \end{boxedverbatim}
    \caption[Example unit test]{Example unit test for testing the lexer
        functionality of the query subsystem.}
    \label{fig:example-unit-test}
\end{figure}
% }}}

I used integration testing to test the public API of the library component of
\texttt{tagfs}. This is the API that both the command line interface and the
FUSE file system use to operate the tag database. Within these tests, the
database is created entirely from scratch and stored in memory, so that the
tests are easily reproducible. In Rust, integration tests are entirely separate
crates\footnote{``Crate'' is the Rust specific term for compilation unit.}, so
by design they cannot access anything that normal consumers of the library
cannot.

I also used integration testing to test the FUSE file system. This is done by
creating a new temporary database and a temporary mount point. The file system
is then mounted at this mount point using the new database. The database is
then modified using the public API methods exposed by the \texttt{tagfs}
library. These updates are then seen reflected in the file system where they
are programmatically checked to ensure correctness. To make it easier to write
tests that check the file system, I developed a series of macros (similar to
the built in \verb@assert!@ and \verb@assert_eq!@ macros) to test file system
state. These include \verb@assert_symlink!@ to ensure that a path exists and is
a symlink, and \verb@assert_dir_children!@ to ensure that a directory is
located at the given path and that the directory's children in the file system
match those given to the macro.

\todo{Full system testing of filesystem. Custom macros}
